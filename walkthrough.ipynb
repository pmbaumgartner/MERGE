{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE Algorithm Walkthrough\n",
    "This is a reimplementation of the MERGE algorithm. My hope is that by proceeding through the algorithm in a linear fashion, the order of operations will be clearer and will facilitate a refactoring into more generalized code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "When the model is initialized, it needs three things: a pre-processed corpus, a gapsize, and the number of iterations. We'll use the corpus included in the original implementation, which is \"[...] a combination of the Santa Barbara Corpus of Spoken American English and the spoken component of the ICE Canada corpus\"\n",
    "\n",
    "> Du Bois, John W., Wallace L. Chafe, Charles Meyers, Sandra A. Thompson, Nii Martey, and Robert Englebretson (2005). Santa Barbara corpus of spoken American English. Philadelphia: Linguistic Data Consortium.\n",
    "\n",
    "> Newman, John and Georgie Columbus (2010). The International Corpus of English – Canada. Edmonton, Alberta: University of Alberta.\n",
    "\n",
    "\n",
    "Note that these are dialog corpuses, so each line is often referred to as a `turn`. They've also been pre-processed with lowercasing, and punctuation replaced with alphanumeric substitutions (`_` --> `undrscr`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines (turns): 62,676\n",
      "Tokens (corpus_size in code): 844,510\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "# these are from the original implementation, but we get the same line count without them\n",
    "# so they're here for reference\n",
    "delimiters = set([\" \", \".\", \",\", \"?\", \";\", \":\", \"!\", \"\\r\", \"\\n\"])\n",
    "\n",
    "corpus = []\n",
    "for txt_file in Path(\"Combined_corpus/\").glob(\"*.TXT\"):\n",
    "    for line in txt_file.read_text().split(\"\\n\"):\n",
    "        if line:\n",
    "            corpus.append(line.split(\" \"))\n",
    "\n",
    "corpus_size = len(list(chain.from_iterable(corpus)))\n",
    "print(\n",
    "    f\"Lines (turns): {len(corpus):,}\",\n",
    ")\n",
    "print(f\"Tokens (corpus_size in code): {corpus_size:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then store the tokens in the special data structure\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product\n",
    "from typing import (\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    NamedTuple,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    DefaultDict,\n",
    "    Counter as CounterType,\n",
    "    NewType,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create some simple data structures\n",
    "\n",
    "\n",
    "class Word(NamedTuple):\n",
    "    wordstr: str\n",
    "    position: int\n",
    "\n",
    "\n",
    "# Lexeme can be used to store n-grams as the algorithm works\n",
    "# plus we need to hash it so we use an expanding tuple\n",
    "\n",
    "\n",
    "class Lexeme(NamedTuple):\n",
    "    word: Tuple[Word, ...]\n",
    "    token_index: int\n",
    "\n",
    "\n",
    "# These are not in original code but we refer to them alot, so they will\n",
    "# be helpful as types.\n",
    "\n",
    "LineIndex = NewType(\"LineIndex\", int)\n",
    "TokenIndex = NewType(\"TokenIndex\", int)\n",
    "\n",
    "# In the original code there is a Lexemes object that stores all this data\n",
    "# in an effort to flatten and linearize the code, I wont define or use those\n",
    "# objects here\n",
    "\n",
    "# Lexemes\n",
    "_lexemes_to_locations: DefaultDict[\n",
    "    Lexeme, Set[Tuple[LineIndex, TokenIndex]]\n",
    "] = defaultdict(set)\n",
    "_locations_to_lexemes: DefaultDict[LineIndex, Dict[TokenIndex, Lexeme]] = defaultdict(\n",
    "    dict\n",
    ")\n",
    "_locations_to_locations: Dict[\n",
    "    Tuple[LineIndex, TokenIndex], Tuple[LineIndex, TokenIndex]\n",
    "] = {}\n",
    "turn_lengths: CounterType[int] = Counter()\n",
    "\n",
    "# NOTE: This is a Counter in original code, but doesn't use counter methods.\n",
    "# and is typed as a regular dict here\n",
    "_lexemes_to_freqs: Dict[Lexeme, int] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Lexeme(word=(Word(wordstr='the', position=0),), token_index=0), 35262),\n",
       " (Lexeme(word=(Word(wordstr='and', position=0),), token_index=0), 26174),\n",
       " (Lexeme(word=(Word(wordstr='i', position=0),), token_index=0), 21216),\n",
       " (Lexeme(word=(Word(wordstr='to', position=0),), token_index=0), 19509),\n",
       " (Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), 18314)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line_ix, line in enumerate(corpus):\n",
    "    for word_ix, word in enumerate(line):\n",
    "        lexeme = Lexeme(word=(Word(word, 0),), token_index=0)\n",
    "        loc: Tuple[LineIndex, TokenIndex] = (line_ix, word_ix)\n",
    "        _lexemes_to_locations[lexeme].add(loc)\n",
    "        _locations_to_lexemes[line_ix][word_ix] = lexeme\n",
    "        _locations_to_locations[\n",
    "            loc\n",
    "        ] = loc  # from original code, not sure what this does\n",
    "        turn_lengths[line_ix] += 1  # TODO: Check out where this is used\n",
    "\n",
    "# Lexemes.count_frequencies()\n",
    "_lexemes_to_freqs = {k: len(v) for k, v in _lexemes_to_locations.items()}\n",
    "\n",
    "# We could double check this as a counter now\n",
    "Counter(_lexemes_to_freqs).most_common(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Data\n",
    "Bigram data is stored in another object called `Bigrams`. There is a lot of logic in here we are going to separate out. \n",
    "\n",
    "We also need to declare a `gapsize` at this point, which allows for MWEs with discontinuity (e.g. `in _ of`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gapsize = NewType(\"Gapsize\", int)\n",
    "\n",
    "gapsize = Gapsize(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "class Bigram(NamedTuple):\n",
    "    el1: Lexeme\n",
    "    el2: Lexeme\n",
    "    gapsize: int\n",
    "\n",
    "\n",
    "def defaultdict_set_factory():\n",
    "    return defaultdict(set)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BigramData:\n",
    "    bigrams_to_freqs: CounterType[Bigram] = field(default_factory=Counter)\n",
    "    bigrams_to_locations: Dict[Bigram, Set[Tuple[LineIndex, TokenIndex]]] = field(\n",
    "        default_factory=lambda: defaultdict(set)\n",
    "    )\n",
    "    left_lex_to_bigrams: Dict[Bigram, Set[Tuple[Lexeme, Gapsize]]] = field(\n",
    "        default_factory=lambda: defaultdict(set)\n",
    "    )\n",
    "    right_lex_to_bigrams: Dict[Bigram, Set[Tuple[Lexeme, Gapsize]]] = field(\n",
    "        default_factory=lambda: defaultdict(set)\n",
    "    )\n",
    "    type_count: int = 0\n",
    "\n",
    "\n",
    "# Maybe put these in a container\n",
    "\n",
    "initial_bigrams = BigramData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_gap_product = product(*(turn_lengths.items(), range(gapsize + 1)))\n",
    "# This could just be a nested for loop, but we find the cartesian product\n",
    "# we essentially want to loop over each line and each gapsize\n",
    "for (turnindex, turnlength), curr_gapsize in line_gap_product:\n",
    "    # rightmost_leftedge in code\n",
    "    last = turnlength - curr_gapsize - 1\n",
    "    # last token that can be part of a bigram. Think if gapsize = 0,\n",
    "    # the the second to last token will be the first element of\n",
    "    # the final bigram\n",
    "    # NOTE: we arent subtracting (-2) beacuse length is already\n",
    "    # iterables + 1\n",
    "    for ix in range(last):\n",
    "        _left = _locations_to_lexemes[turnindex][ix]\n",
    "        _right = _locations_to_lexemes[turnindex][ix + curr_gapsize + 1]\n",
    "        _location = _locations_to_locations.get((turnindex, ix), (turnindex, ix))\n",
    "        bgr = Bigram(el1=_left, el2=_right, gapsize=curr_gapsize)\n",
    "        if bgr not in initial_bigrams.bigrams_to_freqs:\n",
    "            initial_bigrams.type_count += 1\n",
    "            initial_bigrams.left_lex_to_bigrams[(_left, curr_gapsize)].add(bgr)\n",
    "            initial_bigrams.right_lex_to_bigrams[(_right, curr_gapsize)].add(bgr)\n",
    "        initial_bigrams.bigrams_to_freqs[bgr] += 1\n",
    "        initial_bigrams.bigrams_to_locations[bgr].add(_location)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Table\n",
    "The canddiate table is a pandas dataframe that is used to store frequency information. In the original implementation it's something like a paged data frame with multiple `tables`. I don't see the reason for this so we'll simplify the implementation into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for index, (bgr, freq) in enumerate(initial_bigrams.bigrams_to_freqs.items()):\n",
    "    # Look up the frequency of each element of the bigrams\n",
    "    el1_freq = _lexemes_to_freqs[bgr.el1]\n",
    "    el2_freq = _lexemes_to_freqs[bgr.el2]\n",
    "    row = {\"bgr\": bgr, \"bgr_freq\": freq, \"el1_freq\": el1_freq, \"el2_freq\": el2_freq}\n",
    "    data.append(row)\n",
    "\n",
    "table = pd.DataFrame(data).set_index(\"bgr\")\n",
    "table_og = table.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bgr_freq</th>\n",
       "      <th>el1_freq</th>\n",
       "      <th>el2_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bgr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(((Word(wordstr='so', position=0),), 0), ((Word(wordstr='you', position=0),), 0), 0)</th>\n",
       "      <td>351</td>\n",
       "      <td>6702</td>\n",
       "      <td>18314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(((Word(wordstr='you', position=0),), 0), ((Word(wordstr='donundrscrt', position=0),), 0), 0)</th>\n",
       "      <td>513</td>\n",
       "      <td>18314</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(((Word(wordstr='donundrscrt', position=0),), 0), ((Word(wordstr='need', position=0),), 0), 0)</th>\n",
       "      <td>50</td>\n",
       "      <td>3476</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(((Word(wordstr='need', position=0),), 0), ((Word(wordstr='to', position=0),), 0), 0)</th>\n",
       "      <td>205</td>\n",
       "      <td>577</td>\n",
       "      <td>19509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(((Word(wordstr='to', position=0),), 0), ((Word(wordstr='go', position=0),), 0), 0)</th>\n",
       "      <td>583</td>\n",
       "      <td>19509</td>\n",
       "      <td>2231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    bgr_freq  el1_freq  \\\n",
       "bgr                                                                      \n",
       "(((Word(wordstr='so', position=0),), 0), ((Word...       351      6702   \n",
       "(((Word(wordstr='you', position=0),), 0), ((Wor...       513     18314   \n",
       "(((Word(wordstr='donundrscrt', position=0),), 0...        50      3476   \n",
       "(((Word(wordstr='need', position=0),), 0), ((Wo...       205       577   \n",
       "(((Word(wordstr='to', position=0),), 0), ((Word...       583     19509   \n",
       "\n",
       "                                                    el2_freq  \n",
       "bgr                                                           \n",
       "(((Word(wordstr='so', position=0),), 0), ((Word...     18314  \n",
       "(((Word(wordstr='you', position=0),), 0), ((Wor...      3476  \n",
       "(((Word(wordstr='donundrscrt', position=0),), 0...       577  \n",
       "(((Word(wordstr='need', position=0),), 0), ((Wo...     19509  \n",
       "(((Word(wordstr='to', position=0),), 0), ((Word...      2231  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating the algorithm\n",
    "\n",
    "Now the fun begins. The first thing we do is calculate the 'winner' from the bigram tables by calculating the log-likelihood for each bigram. The bigram with the highest log-likelihood is the winner, and will then be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate this specific implementation of log-likelihood\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "obsA = table[\"bgr_freq\"]\n",
    "obsB = table[\"el1_freq\"] - table[\"bgr_freq\"]\n",
    "obsC = table[\"el2_freq\"] - table[\"bgr_freq\"]\n",
    "obsD = corpus_size - (obsA + obsB + obsC)\n",
    "\n",
    "expA = table[\"el2_freq\"] / corpus_size * table[\"el1_freq\"]\n",
    "expB = (corpus_size - table[\"el2_freq\"]) / corpus_size * table[\"el1_freq\"]\n",
    "expC = table[\"el2_freq\"] / corpus_size * (corpus_size - table[\"el1_freq\"])\n",
    "expD = (\n",
    "    (corpus_size - table[\"el2_freq\"]) / corpus_size * (corpus_size - table[\"el1_freq\"])\n",
    ")\n",
    "\n",
    "llA = obsA * np.log1p(obsA / expA)\n",
    "\n",
    "real_vals = np.where(obsB != 0)[0]\n",
    "llB = np.zeros(len(table))\n",
    "llB[real_vals] = obsB[real_vals] * np.log(obsB[real_vals] / expB[real_vals])\n",
    "\n",
    "real_vals = np.where(obsC != 0)[0]\n",
    "llC = np.zeros(len(table))\n",
    "llC[real_vals] = obsC[real_vals] * np.log(obsC[real_vals] / expC[real_vals])\n",
    "\n",
    "llD = obsD * np.log(obsD / expD)\n",
    "\n",
    "log_likelihood = 2 * (llA + llB + llC + llD)\n",
    "negs = np.where(llA < 0)[0]\n",
    "log_likelihood[negs] = log_likelihood[negs] * -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bgr\n",
       "(((Word(wordstr='i', position=0),), 0), ((Word(wordstr='donundrscrt', position=0),), 0), 0)     9586.453385\n",
       "(((Word(wordstr='i', position=0),), 0), ((Word(wordstr='mean', position=0),), 0), 0)            9674.049457\n",
       "(((Word(wordstr='i', position=0),), 0), ((Word(wordstr='think', position=0),), 0), 0)           9868.626038\n",
       "(((Word(wordstr='mhh', position=0),), 0), ((Word(wordstr='hmm', position=0),), 0), 0)          10541.817803\n",
       "(((Word(wordstr='you', position=0),), 0), ((Word(wordstr='know', position=0),), 0), 0)         28104.733018\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: This was checked against the implementation + was correct.\n",
    "\n",
    "log_likelihood.sort_values().tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='know', position=0),), token_index=0), gapsize=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner_info: Bigram = log_likelihood.idxmax()\n",
    "winner_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "We now create the merged token, which will then be substituted into the original corpus where it occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_bigram_to_lexeme(bigram: Bigram) -> Lexeme:\n",
    "    el1_words = list(bigram.el1.word)\n",
    "    el2_words = [\n",
    "        Word(wordstr=word, position=(pos + bigram.gapsize + 1))\n",
    "        for (word, pos) in bigram.el2.word\n",
    "    ]\n",
    "    all_words = sorted(el1_words + el2_words, key=lambda word: word[1])\n",
    "    new_lexeme = Lexeme(word=tuple(all_words), token_index=0)\n",
    "    return new_lexeme\n",
    "\n",
    "\n",
    "merge_token = merge_bigram_to_lexeme(winner_info)\n",
    "merge_tracker: List[Lexeme] = []  # This is a Dict[iter, Lexeme] in the source\n",
    "merge_tracker.append(merge_token)\n",
    "merge_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Updater\n",
    "\n",
    "This is the most complex part of the algorithm. The steps of the bigram updater are:\n",
    "\n",
    "- `Lexemes.set_merge_token` - creates `satellite_lexemes` a dict of position to new `Lexeme` with `token_index`=`position`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0),\n",
       " 1: Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=1)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satellite_lexemes = {0: merge_token}\n",
    "for wordstr, position in merge_token.word:\n",
    "    if position > 0:\n",
    "        satellite_lex = Lexeme(word=merge_token.word, token_index=position)\n",
    "        satellite_lexemes[position] = satellite_lex\n",
    "\n",
    "satellite_lexemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through winner locations for this next part\n",
    "\n",
    "ContextPosition = NewType(\"ContextPosition\", int)\n",
    "SatellitePosition = NewType(\"SatellitePosition\", int)\n",
    "\n",
    "\n",
    "class ContextPos(NamedTuple):\n",
    "    context_pos: ContextPosition\n",
    "    satellite_pos: SatellitePosition\n",
    "    gapsize: Gapsize\n",
    "    merge_token_leftanchor: TokenIndex\n",
    "\n",
    "\n",
    "merged_satellite_positions: Dict[Tuple[LineIndex, SatellitePosition], TokenIndex] = {}\n",
    "\n",
    "winner_locations = initial_bigrams.bigrams_to_locations[winner_info]\n",
    "merge_token_count = 0\n",
    "\n",
    "new_bigrams = BigramData()\n",
    "conflicting_bigrams = BigramData()\n",
    "\n",
    "for (line_ix, word_ix) in winner_locations:\n",
    "    # Check if it's among the existing conflicting bigrams\n",
    "    # the first iteration we wont have this, so we'll come back to it\n",
    "    # TODO: `bigram_is_among_existing_conflicting_bigrams`\n",
    "    # Which is just a lookup of the winner in the `conflicting bigrams` table\n",
    "    if (line_ix, word_ix) in conflicting_bigrams.bigrams_to_locations[winner_info]:\n",
    "        # BigramUpdater.bigram_is_among_existing_conflicting_bigrams\n",
    "        continue\n",
    "\n",
    "    merge_token_count += 1\n",
    "\n",
    "    satellite_positions = [word_ix + word.position for word in merge_token.word]\n",
    "\n",
    "    # context_positions = BigramUpdater.context_pos_manager.generate_positions_around_satellites\n",
    "    context_positions: List[ContextPos] = []\n",
    "    curr_turn_length = turn_lengths[line_ix]\n",
    "    for satellite_position in satellite_positions:\n",
    "        for curr_gapsize in range(gapsize + 1):\n",
    "            left_context_position = satellite_position - curr_gapsize - 1\n",
    "            if left_context_position >= 0:\n",
    "                curr_contextpos = ContextPos(\n",
    "                    left_context_position, satellite_position, curr_gapsize, word_ix\n",
    "                )\n",
    "                context_positions.append(curr_contextpos)\n",
    "            right_context_position = satellite_position + curr_gapsize + 1\n",
    "            if right_context_position < curr_turn_length:\n",
    "                curr_contextpos = ContextPos(\n",
    "                    right_context_position, satellite_position, curr_gapsize, word_ix\n",
    "                )\n",
    "                context_positions.append(curr_contextpos)\n",
    "\n",
    "    # BigramUpdater.vicinity_lex_manager.create_bigrams_with_lexemes_surrounding_satellites(\n",
    "    # merged_satellite_positions is an empty dict on iter = 0 but referenced here\n",
    "    for context_position_info in context_positions:\n",
    "\n",
    "        # Context_position_info gets unpacked in original code if you're looking for those variables\n",
    "        satellite_position = context_position_info.satellite_pos\n",
    "        context_pos = context_position_info.context_pos\n",
    "        if context_pos in satellite_positions:\n",
    "            # TODO: Investigate what this does, without it a bunch\n",
    "            # of token counts are added\n",
    "            continue\n",
    "        # (self.premerge_lexeme, self.premerge_leftanchor) = self.all_lexemes.get_lexeme(self.turn_number, self.satellite_position)\n",
    "        # these _private variables are a re-implementation of Lexemes.get_lexeme\n",
    "        _ref_lexeme = _locations_to_lexemes[line_ix][satellite_position]\n",
    "        _ref_pos_in_turn = satellite_position - _ref_lexeme.token_index\n",
    "        _leftanchor_lexeme = _locations_to_lexemes[line_ix][_ref_pos_in_turn]\n",
    "\n",
    "        premerge_lexeme = _leftanchor_lexeme\n",
    "        premerge_leftanchor = _ref_pos_in_turn\n",
    "        context_loc = (line_ix, context_pos)\n",
    "\n",
    "        if context_loc in merged_satellite_positions:\n",
    "            context_lexeme = merge_token\n",
    "            context_ix = merged_satellite_positions[context_loc]\n",
    "        else:\n",
    "            # TODO: another re-implementation of Lexemes.get_lexeme(turn, word)\n",
    "            _context_ref_lexeme = _locations_to_lexemes[line_ix][context_pos]\n",
    "            _context_ref_pos_in_turn = context_pos - _context_ref_lexeme.token_index\n",
    "            _context_leftanchor_lexeme = _locations_to_lexemes[line_ix][\n",
    "                _context_ref_pos_in_turn\n",
    "            ]\n",
    "            context_lexeme = _context_leftanchor_lexeme\n",
    "            context_ix = _context_ref_pos_in_turn\n",
    "\n",
    "        # BigramUpdater.create_new_bigram()\n",
    "        left_context = context_ix < word_ix\n",
    "        if left_context:\n",
    "            # all_lexemes.get_extant_loc_object(turn_number, context_ix )\n",
    "            location_tuple = _locations_to_locations.get(\n",
    "                (line_ix, context_ix), (line_ix, context_ix)\n",
    "            )\n",
    "            gap_between_anchors = word_ix - context_ix - 1\n",
    "            _left, _right = context_lexeme, merge_token\n",
    "        else:  # right context\n",
    "            location_tuple = _locations_to_locations.get(\n",
    "                (line_ix, word_ix), (line_ix, word_ix)\n",
    "            )\n",
    "            gap_between_anchors = (\n",
    "                context_ix - word_ix - 1\n",
    "            )  # order reversed from left_context\n",
    "            _left, _right = merge_token, context_lexeme\n",
    "        # reimplemtation of Bigrams.save_bigram_data\n",
    "        bgr = Bigram(el1=_left, el2=_right, gapsize=gap_between_anchors)\n",
    "        if bgr not in new_bigrams.bigrams_to_freqs:\n",
    "            new_bigrams.type_count += 1\n",
    "            new_bigrams.left_lex_to_bigrams[(_left, gap_between_anchors)].add(bgr)\n",
    "            new_bigrams.right_lex_to_bigrams[(_right, gap_between_anchors)].add(bgr)\n",
    "        new_bigrams.bigrams_to_freqs[bgr] += 1\n",
    "        new_bigrams.bigrams_to_locations[bgr].add(location_tuple)\n",
    "\n",
    "        # BigramUpdater.create_conflicting_bigram()\n",
    "        left_context = context_ix < premerge_leftanchor\n",
    "        if left_context:\n",
    "            # all_lexemes.get_extant_loc_object(turn_number, context_ix )\n",
    "            location_tuple = _locations_to_locations.get(\n",
    "                (line_ix, context_ix), (line_ix, context_ix)\n",
    "            )\n",
    "            gap_between_anchors = premerge_leftanchor - context_ix - 1\n",
    "            _left, _right = context_lexeme, premerge_lexeme\n",
    "        else:  # right context\n",
    "            location_tuple = _locations_to_locations.get(\n",
    "                (line_ix, premerge_leftanchor), (line_ix, premerge_leftanchor)\n",
    "            )\n",
    "            gap_between_anchors = (\n",
    "                context_ix - premerge_leftanchor - 1\n",
    "            )  # order reversed from left_context\n",
    "            _left, _right = premerge_lexeme, context_lexeme\n",
    "        # reimplemtation of Bigrams.save_bigram_data\n",
    "        bgr = Bigram(el1=_left, el2=_right, gapsize=gap_between_anchors)\n",
    "        if bgr not in conflicting_bigrams.bigrams_to_freqs:\n",
    "            conflicting_bigrams.type_count += 1\n",
    "            conflicting_bigrams.left_lex_to_bigrams[(_left, gap_between_anchors)].add(\n",
    "                bgr\n",
    "            )\n",
    "            conflicting_bigrams.right_lex_to_bigrams[(_right, gap_between_anchors)].add(\n",
    "                bgr\n",
    "            )\n",
    "        conflicting_bigrams.bigrams_to_freqs[bgr] += 1\n",
    "        conflicting_bigrams.bigrams_to_locations[bgr].add(location_tuple)\n",
    "\n",
    "    for satellite_position in satellite_positions:\n",
    "        merged_satellite_positions[(line_ix, satellite_position)] = word_ix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conflicting_bigrams.bigrams_to_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_bigrams_diff: 0, conflicting_bigrams_diff: 0\n"
     ]
    }
   ],
   "source": [
    "type_count_check = (new_bigrams.type_count, conflicting_bigrams.type_count)\n",
    "original_impl = (3531, 5207)\n",
    "print(\n",
    "    f\"new_bigrams_diff: {original_impl[0]-type_count_check[0]}, conflicting_bigrams_diff: {original_impl[1]-type_count_check[1]}\"\n",
    ")\n",
    "# after iter 1\n",
    "# AssertionError: new_bigrams_diff: 2, conflicting_bigrams_diff: 6\n",
    "# TODO: Come back to this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflicting_bigrams.bigrams_to_locations[winner_info]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='know', position=0),), token_index=0), gapsize=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='i', position=0),), token_index=0), gapsize=1),\n",
       "  305),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='know', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='i', position=0),), token_index=0), gapsize=0),\n",
       "  305),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='and', position=0),), token_index=0), gapsize=1),\n",
       "  238),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='know', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='and', position=0),), token_index=0), gapsize=0),\n",
       "  238),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='what', position=0),), token_index=0), gapsize=1),\n",
       "  235)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflicting_bigrams.bigrams_to_freqs.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0), el2=Lexeme(word=(Word(wordstr='i', position=0),), token_index=0), gapsize=1),\n",
       "  610),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0), el2=Lexeme(word=(Word(wordstr='and', position=0),), token_index=0), gapsize=1),\n",
       "  476),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0), el2=Lexeme(word=(Word(wordstr='what', position=0),), token_index=0), gapsize=1),\n",
       "  470),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='and', position=0),), token_index=0), el2=Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0), gapsize=0),\n",
       "  440),\n",
       " (Bigram(el1=Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0), el2=Lexeme(word=(Word(wordstr='the', position=0),), token_index=0), gapsize=1),\n",
       "  358)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bigrams.bigrams_to_freqs.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=0),\n",
       " 1: Lexeme(word=(Word(wordstr='you', position=0), Word(wordstr='know', position=1)), token_index=1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satellite_lexemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigramUpdater.update_all_lexemes_with_merge_tokens()\n",
    "# (after main_control_loop)\n",
    "_lexemes_to_freqs[merge_token] = merge_token_count\n",
    "for (line_ix, satellite_pos), word_ix in merged_satellite_positions.items():\n",
    "    # all_lexemes.add_merge_token(turn, satellite_pos, merge_token_leftanchor)\n",
    "    loc = _locations_to_locations[(line_ix, satellite_pos)]\n",
    "    satellite_lexeme = satellite_lexemes[satellite_pos - word_ix]\n",
    "    _lexemes_to_locations[satellite_lexeme].add(loc)\n",
    "    _locations_to_lexemes[line_ix][satellite_pos] = satellite_lexeme\n",
    "\n",
    "    # TODO: Look at this more. I believe this is where the original bigram elements get replaced with\n",
    "    # But both the original and the satellite will get replaced with the bigram,\n",
    "    # which seems different than the original paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency updater\n",
    "\n",
    "Now we handle the frequency updater, which we use for handling the statistics and log log_likelihood.\n",
    "\n",
    "```python\n",
    "self.frequency_updater.set_new_freqs_for_elements_in_winner(\n",
    "    self.all_lexemes, self.winner_info, self.merge_token_count\n",
    ")\n",
    "self.frequency_updater.process_bgrs_with_same_element_types_as_winner(\n",
    "    self.gapsize, self.all_tables, self.all_bigrams\n",
    ")\n",
    "self.frequency_updater.add_new_bigrams(self.new_bigrams)\n",
    "self.frequency_updater.update_conflicting_bigram_freqs(\n",
    "    self.conflicting_bigrams\n",
    ")\n",
    "self.frequency_updater.remove_winner()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size -= merge_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_lexemes_to_freqs[winner_info.el1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract total count from each element\n",
    "# FrequencyUpdater.set_new_freqs_for_elements_in_winner\n",
    "# NOTE: RUnning this several times will continually subtract\n",
    "el1_freq = _lexemes_to_freqs[winner_info.el1]\n",
    "new_el1_freq = el1_freq - merge_token_count\n",
    "_lexemes_to_freqs[winner_info.el1] = new_el1_freq\n",
    "\n",
    "el2_freq = _lexemes_to_freqs[winner_info.el2]\n",
    "new_el2_freq = el2_freq - merge_token_count\n",
    "_lexemes_to_freqs[winner_info.el2] = new_el2_freq\n",
    "\n",
    "\n",
    "# FrequencyUpdater.process_bgrs_with_same_element_types_as_winner\n",
    "# get bigrams with elements of winner\n",
    "test = set()\n",
    "for curr_gapsize in range(gapsize + 1):\n",
    "    el1_left_pos = initial_bigrams.left_lex_to_bigrams[(winner_info.el1, curr_gapsize)]\n",
    "    el1_right_pos = initial_bigrams.right_lex_to_bigrams[\n",
    "        (winner_info.el1, curr_gapsize)\n",
    "    ]\n",
    "    el2_left_pos = initial_bigrams.left_lex_to_bigrams[(winner_info.el2, curr_gapsize)]\n",
    "    el2_right_pos = initial_bigrams.right_lex_to_bigrams[\n",
    "        (winner_info.el2, curr_gapsize)\n",
    "    ]\n",
    "\n",
    "    cols = [\"el1_freq\", \"el2_freq\"] * 2\n",
    "    bigram_pos = [\n",
    "        el1_left_pos,\n",
    "        el1_right_pos,\n",
    "        el2_left_pos,\n",
    "        el2_right_pos,\n",
    "    ]\n",
    "    for bigrams, col in zip(bigram_pos, cols):\n",
    "        for bigram in bigrams:\n",
    "            value = new_el1_freq if col.startswith(\"el1\") else new_el2_freq\n",
    "            table.at[bigram, col] = value\n",
    "    # This can definitely be optimized if we could groupby el1, el2\n",
    "    # rather than a lookup at each individual row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_el1_freq, original_el2_freq = (13662, 2471)\n",
    "assert (new_el1_freq, new_el2_freq) == (original_el1_freq, original_el2_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequencyUpdater.add_new_bigrams\n",
    "from copy import deepcopy\n",
    "\n",
    "# Going to make a copy just in case we want to validate against the original\n",
    "collected_bigrams: BigramData = deepcopy(initial_bigrams)\n",
    "\n",
    "# Bigrams.add(new_bigrams)\n",
    "# for all these loops: collected = initial (lookup) + new (in iter)\n",
    "for index, (bgr, freq) in enumerate(new_bigrams.bigrams_to_freqs.items()):\n",
    "    collected_bigrams.bigrams_to_freqs[bgr] += freq\n",
    "    curr_locs = initial_bigrams.bigrams_to_locations[bgr]\n",
    "    collected_bigrams.bigrams_to_locations[bgr] = curr_locs.union(\n",
    "        new_bigrams.bigrams_to_locations[bgr]\n",
    "    )\n",
    "\n",
    "for (el1, curr_gapsize), bigrams in new_bigrams.left_lex_to_bigrams.items():\n",
    "    curr_left_lex_to_bigrams = initial_bigrams.left_lex_to_bigrams[(el1, curr_gapsize)]\n",
    "    collected_bigrams.left_lex_to_bigrams[\n",
    "        (el1, curr_gapsize)\n",
    "    ] = curr_left_lex_to_bigrams.union(bigrams)\n",
    "\n",
    "for (el2, curr_gapsize), bigrams in new_bigrams.right_lex_to_bigrams.items():\n",
    "    curr_right_lex_to_bigrams = initial_bigrams.right_lex_to_bigrams[\n",
    "        (el2, curr_gapsize)\n",
    "    ]\n",
    "    collected_bigrams.right_lex_to_bigrams[\n",
    "        (el2, curr_gapsize)\n",
    "    ] = curr_right_lex_to_bigrams.union(bigrams)\n",
    "\n",
    "collected_bigrams.type_count = len(collected_bigrams.bigrams_to_freqs)\n",
    "\n",
    "new_bigram_records = []\n",
    "for bigram in new_bigrams.bigrams_to_freqs:\n",
    "    new_freq = collected_bigrams.bigrams_to_freqs[bigram]\n",
    "    new_el1_freq = _lexemes_to_freqs[bigram.el1]\n",
    "    new_el2_freq = _lexemes_to_freqs[bigram.el2]\n",
    "    # FrequencyUpdater.new_data_for_tables.push_row\n",
    "    # Which just adds all these to one list - I think I can just append\n",
    "\n",
    "    # FrequencyUpdater.all_tables.add\n",
    "    # This can be made much simpler since we arent doing the broken tables\n",
    "    new_bigram_records.append(\n",
    "        {\n",
    "            \"bgr\": bigram,\n",
    "            \"bgr_freq\": new_freq,\n",
    "            \"el1_freq\": new_el1_freq,\n",
    "            \"el2_freq\": new_el2_freq,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "new_bigram_df = pd.DataFrame(new_bigram_records).set_index(\"bgr\")\n",
    "table = pd.concat((table, new_bigram_df), axis=\"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequencyUpdater.update_conflicting_bigram_freqs\n",
    "# all_bigrams.deduct_freqs(self.conflicting_bigrams)\n",
    "# Here all bigrams is collected_bigrams (which has been modified above)\n",
    "\n",
    "for index, (bgr, freq) in enumerate(conflicting_bigrams.bigrams_to_freqs.items()):\n",
    "    collected_bigrams.bigrams_to_freqs[bgr] -= freq\n",
    "    curr_locs = conflicting_bigrams.bigrams_to_locations[bgr]\n",
    "    for loc in curr_locs:\n",
    "        collected_bigrams.bigrams_to_locations[bgr].remove(loc)\n",
    "    if collected_bigrams.bigrams_to_freqs[bgr] < 1:\n",
    "        del collected_bigrams.bigrams_to_freqs[bgr]\n",
    "        del collected_bigrams.bigrams_to_locations[bgr]\n",
    "        collected_bigrams.left_lex_to_bigrams[(bgr.el1, bgr.gapsize)].remove(bgr)\n",
    "        collected_bigrams.right_lex_to_bigrams[(bgr.el2, bgr.gapsize)].remove(bgr)\n",
    "\n",
    "collected_bigrams.type_count = len(collected_bigrams.bigrams_to_freqs)\n",
    "\n",
    "for bigram in conflicting_bigrams.bigrams_to_freqs:\n",
    "    table.at[bigram, \"bgr_freq\"] = collected_bigrams.bigrams_to_freqs[bigram]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537076"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FrequencyUpdater.remove_winner()\n",
    "del collected_bigrams.bigrams_to_freqs[winner_info]\n",
    "del collected_bigrams.bigrams_to_locations[winner_info]\n",
    "collected_bigrams.left_lex_to_bigrams[(winner_info.el1, winner_info.gapsize)].remove(\n",
    "    winner_info\n",
    ")\n",
    "collected_bigrams.right_lex_to_bigrams[(winner_info.el2, winner_info.gapsize)].remove(\n",
    "    winner_info\n",
    ")\n",
    "\n",
    "collected_bigrams.type_count = len(collected_bigrams.bigrams_to_freqs)\n",
    "\n",
    "# 537076\n",
    "collected_bigrams.type_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**✨ That's an iteration!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('MERGE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21fbefeb060c3a8d876632422c3f72860158d9909c862acd7c1b786118ed66c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
